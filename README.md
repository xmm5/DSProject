# Проект "близкие вакансии"

Реализация проекта была разделена на три части:
1. Выгрузка данных вакансий и резюме.
2. Вычисление меры сходства ваканий и их ранжирование.
3. Реализация REST-сервиса для поиска Топ-10 наиболее релевантный вакансий.

Для реализации проекта используется:
- IDE PyCharm (Community Edition)
- Стандартные библиотеки:
  - concurrent, datetime, hashlib, json, os, random, re, threading, time
- Дополнительные библиотеки:
  - bs4, nltk, numpy, pandas, pickle, pymorphy2, requests

## 1. Выгрузка данных вакансий

Данный раздел состоит из следующих шагов:

1. Выгрузить описания открытых вакансий и резюме из площадки 'hh.ru'.

    Для выгрузки данных с сайта 'hh.ru' используются классы VacancyLoader,
    ResumeLoader и HHApi. Классы VacancyLoader и ResumeLoader реализуют
    скачивание описания вакансий и резюме и кэширование их на диске (для
    возможности работы в offline). Класс HHApi представляет API для скачивания
    данных с сайта 'hh.ru'.

    В процессе загрузки данных в директории проекта будет создана директория
    данных '/data'. Данная директория хранит артефакты генерируемые в процессе
    работы программы. После загрузки описаний вакансий и резюме в директории
    '/data' будут созданы директории:
    - '/resume'
    - '/resume_pages'
    - '/vacancy'
    - '/vacancy_pages'

2. Предобработать текст описаний вакансий и резюме.

    Для обработки текста ваканий и резюме используется класс TextTransformer
    который реализует следующие функции:
    - Фильтрацию текста.
       - Удаление HTML-тегов.
       - Удаление HTML-entities.
       - Удаление ссылок.
       - Удаление специальных смволов.
       - Удаление русских и английских стоп-слов.
    - Токенизацию текста.
       - Для этого используется RegexpTokenizer.
    - Приведение текста к нормальной форме.
       - Лемматизация для русских и английских слов.
       - Cтемминг для русских и английских слов.

    В процессе обработки данных в директории '/data' будет создана директория
    '/nltk', которая будет содержать ресурсы библиотеки nltk.

3. Подготовить датасет и сохранить его на диск в формате pickle.

    Для подготовки DataFrame'ов, содержащих данные вакансий и резюме,
    используется класс DataParser. Данный класс реализует следующие функции:
    - Чтение описания ваканий и резюме с диска.
    - Парсинг данных ваканий и резюме.
    - Формирование новой структуры данных для описания ваканий и резюме на базе
      DataFrame'а.

    В процессе обработки данных в директории '/data' будут созданы файлы:
    - 'categories.json'
    - 'resume.pkl5'
    - 'vacancy.pkl5'

## 2. Вычисление меры сходства ваканий и их ранжирование

TODO: В работе.

## 3. Реализация REST-сервиса для поиска Топ-10 наиболее релевантный вакансий

TODO: В работе.

